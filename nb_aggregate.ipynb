{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seungwoong.ha/anaconda3/envs/collmind/lib/python3.11/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/seungwoong.ha/anaconda3/envs/collmind/lib/python3.11/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/seungwoong.ha/anaconda3/envs/collmind/lib/python3.11/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/seungwoong.ha/anaconda3/envs/collmind/lib/python3.11/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/seungwoong.ha/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from proj_utils import *\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import gc\n",
    "import time\n",
    "from os.path import join\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict, Counter\n",
    "from itertools import combinations\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sklearn as sk\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from scipy.optimize import curve_fit\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import mpltern\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual\n",
    "\n",
    "#import bertopic\n",
    "from proj_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_start_date = datetime(2012, 5, 28, 0, 0)  ## Monday\n",
    "\n",
    "collection_name = 'Gatewaypundit'\n",
    "model_name = MODEL_NAMES[collection_name]\n",
    "\n",
    "is_global = False\n",
    "\n",
    "start_date, end_date = DATE_RANGES[collection_name]\n",
    "if is_global:\n",
    "    num_topics = NUM_TOPICS['global']\n",
    "else:\n",
    "    num_topics = NUM_TOPICS[collection_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_save(tmp_df, comment_date_cutoff, is_global=False):\n",
    "\n",
    "    comment_createdAt_list = []\n",
    "    comment_id_list = []\n",
    "    comment_topics_list = []\n",
    "    comment_embeddings_list = []\n",
    "    comment_sentiments_list = []\n",
    "    topic_freq = Counter({i: 0 for i in range(-1, num_topics)})\n",
    "    \n",
    "    for article in tmp_df.itertuples():\n",
    "        filtered_index = np.where(np.array([(comment_createdAt - article.createdAt).days for comment_createdAt in article.comment_createdAt]) < comment_date_cutoff)[0]\n",
    "        comment_createdAt_list.append([article.comment_createdAt[i] for i in filtered_index])\n",
    "        comment_id_list.append([article.comment_id[i] for i in filtered_index])\n",
    "        if is_global:\n",
    "            comment_topics = [article.comment_topics_global[i] for i in filtered_index]\n",
    "        else:\n",
    "            comment_topics = [article.comment_topics[i] for i in filtered_index]\n",
    "        comment_topics_list.append(comment_topics)\n",
    "        topic_freq.update(comment_topics)\n",
    "        comment_embeddings_list.append([article.comment_embeddings[i] for i in filtered_index])\n",
    "        comment_sentiments_list.append([article.comment_sentiments[i] for i in filtered_index])\n",
    "        \n",
    "    tmp_df = tmp_df.assign(comment_id=comment_id_list, comment_topics=comment_topics_list, comment_createdAt=comment_createdAt_list, comment_embeddings=comment_embeddings_list, comment_sentiments=comment_sentiments_list)\n",
    "\n",
    "    topic_dict_tmp_df = tmp_df[['_id', 'topic_num', 'topic_prob', 'comment_topics']]\n",
    "\n",
    "    topic_embedding_list = [[] for _ in range(num_topics+1)] # including -1\n",
    "    topic_sentiment_list = [[] for _ in range(num_topics+1)] # including -1\n",
    "    topic_mean_embedding_list = [[] for _ in range(num_topics+1)] # including -1\n",
    "    topic_mean_sentiment_list = [[] for _ in range(num_topics+1)] # including -1\n",
    "\n",
    "    for article in tmp_df.itertuples():\n",
    "        for i in range(num_topics+1):  # including -1  \n",
    "            comment_index = np.where(np.array(article.comment_topics) == i-1)[0]\n",
    "            if len(comment_index)>0:\n",
    "                topic_embedding_list[i].append(np.array(article.comment_embeddings)[comment_index])\n",
    "                topic_sentiment_list[i].append(np.array(article.comment_sentiments)[comment_index])\n",
    "                    \n",
    "    for i in range(num_topics+1): \n",
    "        assert len(topic_embedding_list[i]) == len(topic_sentiment_list[i])\n",
    "        \n",
    "        if len(topic_embedding_list[i])>0:\n",
    "            averaged_embedding = np.vstack(topic_embedding_list[i]).mean(axis=0)\n",
    "        else:\n",
    "            averaged_embedding = np.zeros(384)  # embedding shape\n",
    "            \n",
    "        if len(topic_sentiment_list[i])>0:\n",
    "            averaged_sentiment = np.vstack(topic_sentiment_list[i]).mean(axis=0)\n",
    "        else:\n",
    "            averaged_sentiment = np.zeros(11)  # sentiment shape\n",
    "        \n",
    "        topic_mean_embedding_list[i] = averaged_embedding \n",
    "        topic_mean_sentiment_list[i] = averaged_sentiment\n",
    "        \n",
    "    # sort topic_freq by key and get values\n",
    "    topic_freq = [topic_freq[key] for key in sorted(topic_freq.keys())]\n",
    "    summary_df = pd.DataFrame({'topic_freq': topic_freq, 'topic_mean_embedding': topic_mean_embedding_list, 'topic_mean_sentiment': topic_mean_sentiment_list})\n",
    "\n",
    "    summary_df_csv = pd.DataFrame({'topic_freq': topic_freq})\n",
    "    tme = pd.DataFrame(topic_mean_embedding_list, columns=['e'+str(i) for i in range(384)])\n",
    "    tms = pd.DataFrame(topic_mean_sentiment_list, columns=['s'+str(i) for i in range(11)])\n",
    "    summary_df_csv = pd.concat([summary_df_csv, tme, tms], axis=1)\n",
    "    \n",
    "    return summary_df, summary_df_csv, topic_dict_tmp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_day_list = ['3d', 'week', 'month']\n",
    "comment_threshold = 10\n",
    "add_dict = {'3d':relativedelta(days=3), 'week':relativedelta(weeks=1), 'month':relativedelta(months=1)}\n",
    "comment_date_cutoff_dict = {'3d': 1, 'week' : 2, 'month': 7}\n",
    "\n",
    "topic_dict = {aggregate_day : {} for aggregate_day in aggregate_day_list}\n",
    "topic_tuple_dict_dict = {aggregate_day : {} for aggregate_day in aggregate_day_list}\n",
    "title_topic_num_list_dict = {aggregate_day : [] for aggregate_day in aggregate_day_list}\n",
    "title_constant_list_dict = {aggregate_day : [] for aggregate_day in aggregate_day_list}\n",
    "\n",
    "tmp_list_df = {aggregate_day : [] for aggregate_day in aggregate_day_list}\n",
    "len_list_dict = {aggregate_day: {} for aggregate_day in aggregate_day_list}\n",
    "\n",
    "# make folders under /data/collmind/article\n",
    "\n",
    "article_folder_name = 'article_global' if is_global else 'article'\n",
    "\n",
    "for aggregate_day in aggregate_day_list:\n",
    "    os.makedirs(join('/data', 'collmind', article_folder_name, collection_name.lower(), aggregate_day), exist_ok=True)\n",
    "    os.makedirs(join('/data', 'collmind', article_folder_name, collection_name.lower(), aggregate_day, 'csv'), exist_ok=True)\n",
    "    os.makedirs(join('/data', 'collmind', article_folder_name, collection_name.lower(), aggregate_day, 'df'), exist_ok=True)\n",
    "\n",
    "final_day = (end_date - relativedelta(days=30)).strftime('%Y-%m-%d')\n",
    "print(final_day)\n",
    "\n",
    "while True:\n",
    "    file_path = join('article', collection_name.lower(), model_name, 'articles_by_day', f'{final_day}.parquet')\n",
    "    if os.path.exists(file_path):\n",
    "        print('aggregate start')\n",
    "        break\n",
    "    else:\n",
    "        print(datetime.now(), 'still waiting')\n",
    "        time.sleep(60 * 60)  # Sleep for 60 minutes\n",
    "\n",
    "\n",
    "file_names = os.listdir(join('article', collection_name.lower(), model_name, 'articles_by_day'))\n",
    "keys = sorted([file_name.split('.')[0] for file_name in file_names], key=lambda x: datetime.strptime(x, '%Y-%m-%d'))\n",
    "start = datetime.strptime(keys[0], '%Y-%m-%d')\n",
    "\n",
    "start_end_dict = {}\n",
    "# 3d\n",
    "for aggregate_day in aggregate_day_list:\n",
    "    start_date = deepcopy(aggregate_start_date)\n",
    "    \n",
    "    if aggregate_day == 'month':\n",
    "        \n",
    "        start_date = start.replace(day=1)\n",
    "        \n",
    "        end_date = (start_date + relativedelta(months=1)).replace(day=1)\n",
    "\n",
    "    else:\n",
    "        while True:\n",
    "            if start_date + add_dict[aggregate_day] > start:\n",
    "                break\n",
    "            else:\n",
    "                start_date += add_dict[aggregate_day]\n",
    "        \n",
    "        end_date = start_date + add_dict[aggregate_day] \n",
    "   \n",
    "    start_end_dict[aggregate_day] = (start_date, end_date)\n",
    "    \n",
    "\n",
    "for key in keys:\n",
    "    print(key)\n",
    "    articles = pd.read_parquet(join('article', collection_name.lower(), model_name, 'articles_by_day', key +'.parquet'))\n",
    "    articles = articles[articles['comment_id'].apply(len) > comment_threshold]\n",
    "    \n",
    "    for aggregate_day in aggregate_day_list:\n",
    "        if datetime.strptime(key, '%Y-%m-%d') >= start_end_dict[aggregate_day][1]:\n",
    "            len_list_dict[aggregate_day][start_end_dict[aggregate_day][0].strftime('%Y-%m-%d')] = len(tmp_list_df[aggregate_day])\n",
    "            tmp_df = pd.concat(tmp_list_df[aggregate_day])\n",
    "            \n",
    "            if len(tmp_df) > 0:\n",
    "                tmp_df = pd.concat(tmp_list_df[aggregate_day])\n",
    "                \n",
    "                if is_global:\n",
    "                    topic_tuple_dict = defaultdict(list)\n",
    "                    for article in tmp_df.itertuples():\n",
    "                        topic_tuple = tuple(sorted(article.topic_num_global[:3]))\n",
    "                        topic_tuple_dict[topic_tuple].append((article._1, article.createdAt.strftime('%Y-%m-%d')))\n",
    "                    topic_tuple_dict_dict[aggregate_day][start_end_dict[aggregate_day][0].strftime('%Y-%m-%d')] = topic_tuple_dict\n",
    "                    \n",
    "                    title_constant_list_dict[aggregate_day].append(len(tmp_df))\n",
    "                    title_topic_num_list_dict[aggregate_day].append(np.zeros((num_topics, 3)))\n",
    "                    for t in range(num_topics):\n",
    "                        for i in range(3):\n",
    "                            title_topic_num_list_dict[aggregate_day][-1][t][i] = len(tmp_df[np.vstack(tmp_df['topic_num_global'])[:, i] == t])\n",
    "                else:\n",
    "                    topic_tuple_dict = defaultdict(list)\n",
    "                    for article in tmp_df.itertuples():\n",
    "                        topic_tuple = tuple(sorted(article.topic_num[:3]))\n",
    "                        topic_tuple_dict[topic_tuple].append((article._1, article.createdAt.strftime('%Y-%m-%d')))\n",
    "                    topic_tuple_dict_dict[aggregate_day][start_end_dict[aggregate_day][0].strftime('%Y-%m-%d')] = topic_tuple_dict\n",
    "                    \n",
    "                    title_constant_list_dict[aggregate_day].append(len(tmp_df))\n",
    "                    title_topic_num_list_dict[aggregate_day].append(np.zeros((num_topics, 3)))\n",
    "                    for t in range(num_topics):\n",
    "                        for i in range(3):\n",
    "                            title_topic_num_list_dict[aggregate_day][-1][t][i] = len(tmp_df[np.vstack(tmp_df['topic_num'])[:, i] == t])\n",
    "                    \n",
    "                summary_df, summary_df_csv, topic_dict_tmp_df = aggregate_save(tmp_df, comment_date_cutoff_dict[aggregate_day], is_global)\n",
    "                topic_dict[aggregate_day][start_end_dict[aggregate_day][0].strftime('%Y-%m-%d')] = topic_dict_tmp_df\n",
    "                \n",
    "                # save summary_df\n",
    "                summary_df.to_parquet(join('/data', 'collmind', article_folder_name, collection_name.lower(), aggregate_day, 'df', start_end_dict[aggregate_day][0].strftime('%Y-%m-%d') + '.parquet'), compression='gzip')\n",
    "                summary_df_csv.to_csv(join('/data', 'collmind', article_folder_name, collection_name.lower(), aggregate_day, 'csv', start_end_dict[aggregate_day][0].strftime('%Y-%m-%d') + '.csv'))\n",
    "                \n",
    "            tmp_list_df[aggregate_day] = []\n",
    "            start_end_dict[aggregate_day] = (start_end_dict[aggregate_day][1], start_end_dict[aggregate_day][1] + add_dict[aggregate_day])\n",
    "        \n",
    "        tmp_list_df[aggregate_day].append(articles)\n",
    "          \n",
    "# process leftovers          \n",
    "          \n",
    "for aggregate_day in aggregate_day_list:\n",
    "        \n",
    "    len_list_dict[aggregate_day][start_end_dict[aggregate_day][0].strftime('%Y-%m-%d')] = len(tmp_list_df[aggregate_day])\n",
    "    tmp_df = pd.concat(tmp_list_df[aggregate_day])\n",
    "    \n",
    "    if len(tmp_df) > 0:\n",
    "        tmp_df = pd.concat(tmp_list_df[aggregate_day])\n",
    "                \n",
    "        topic_tuple_dict = defaultdict(list)\n",
    "        for article in tmp_df.itertuples():\n",
    "            topic_tuple = tuple(sorted(article.topic_num[:3]))\n",
    "            topic_tuple_dict[topic_tuple].append((article._1, article.createdAt.strftime('%Y-%m-%d')))\n",
    "        topic_tuple_dict_dict[aggregate_day][start_end_dict[aggregate_day][0].strftime('%Y-%m-%d')] = topic_tuple_dict\n",
    "        \n",
    "        title_constant_list_dict[aggregate_day].append(len(tmp_df))\n",
    "        title_topic_num_list_dict[aggregate_day].append(np.zeros((num_topics, 3)))\n",
    "        for t in range(num_topics):\n",
    "            for i in range(3):\n",
    "                title_topic_num_list_dict[aggregate_day][-1][t][i] = len(tmp_df[np.vstack(tmp_df['topic_num'])[:, i] == t])\n",
    "        \n",
    "        summary_df, summary_df_csv, topic_dict_tmp_df = aggregate_save(tmp_df, comment_date_cutoff_dict[aggregate_day], is_global)\n",
    "        topic_dict[aggregate_day][start_end_dict[aggregate_day][0].strftime('%Y-%m-%d')] = topic_dict_tmp_df\n",
    "        \n",
    "        # save summary_df\n",
    "        summary_df.to_parquet(join('/data', 'collmind', article_folder_name, collection_name.lower(), aggregate_day, 'df', start_end_dict[aggregate_day][0].strftime('%Y-%m-%d') + '.parquet'))\n",
    "        summary_df_csv.to_csv(join('/data', 'collmind', article_folder_name, collection_name.lower(), aggregate_day, 'csv', start_end_dict[aggregate_day][0].strftime('%Y-%m-%d') + '.csv'))\n",
    "        tmp_list_df[aggregate_day] = []\n",
    "    \n",
    "result_dict = {'topic_tuple_dict_dict': topic_tuple_dict_dict, 'len_list_dict': len_list_dict, 'title_constant_list_dict': title_constant_list_dict, 'title_topic_num_list_dict': title_topic_num_list_dict}\n",
    "\n",
    "# save num_elements\n",
    "with open(join('/data', 'collmind', article_folder_name, collection_name.lower(), 'result_dict.pkl'), 'wb') as f:\n",
    "    pickle.dump(result_dict, f)\n",
    "    \n",
    "for aggregate_day in aggregate_day_list:\n",
    "    with open(join('/data', 'collmind', article_folder_name, collection_name.lower(), aggregate_day, f'topic_dict_{aggregate_day}.pkl'), 'wb') as f:\n",
    "        pickle.dump(topic_dict[aggregate_day], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_parquet(join('article', collection_name.lower(), model_name, 'articles_by_day', '2015-12-03' +'.parquet')).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_parquet(join('article', collection_name.lower(), model_name, 'articles_by_day', '2022-12-12' +'.parquet')).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = pd.read_parquet(join('article', collection_name.lower(), model_name, 'articles_by_day', '2015-02-02' +'.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "collmind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
